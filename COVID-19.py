# -*- coding: utf-8 -*-
"""Copy of Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15RcyI7MfudIioFtzb-89Jh4xNN6vQlEF
"""

import pathlib, os,sys, cv2
import tensorflow as tf
from google.colab import drive
import pandas as pd
import numpy as np
drive.mount('/content/drive')
from PIL import Image
import matplotlib.pyplot as plt
import tensorflow_hub as hub
from collections import defaultdict
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import RMSprop
import tensorflow_hub as hub

from keras.layers import Input, Lambda, Dense, Flatten
from keras.models import Sequential, Model
from keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras import layers

dataset = "drive/My Drive/archive/Covid19-dataset/"

#OS directory 
train_dir  = os.path.join(dataset,'train')
validation_dir = os.path.join(dataset,'test')

#Directory with the covid/normal/viral pneumonia
train_covid_dir = os.path.join(train_dir, 'Covid')
train_normal_dir = os.path.join(train_dir, 'Normal')
train_pneumonia_dir = os.path.join(train_dir, 'Viral Pneumonia')

test_covid_dir = os.path.join(validation_dir, 'Covid')
test_normal_dir = os.path.join(validation_dir, 'Normal')
test_pneumonia_dir = os.path.join(validation_dir, 'Viral Pneumonia')

len_train_set = len(os.listdir(train_covid_dir)) + len(os.listdir( train_normal_dir))+ len(os.listdir( train_pneumonia_dir))
len_test_set = len(os.listdir(test_covid_dir)) + len(os.listdir( test_normal_dir))+ len(os.listdir( test_pneumonia_dir))
total_len = len_train_set + len_test_set

print('total training :', len_train_set )
print('total validation :', len_test_set )

def process_images_train(dir):

   rescaler = ImageDataGenerator(
      rescale = 1.0/255.,
      shear_range = 0.2,
      zoom_range = 0.2,
      fill_mode = 'nearest'    )
   
   processed_images = rescaler.flow_from_directory(
      dir,
      target_size = (224,224),
      follow_links = False,
      batch_size = 32
   
      )
   return processed_images
def process_images_validate(dir):

   rescaler = ImageDataGenerator(
      rescale = 1.0/255.,
      shear_range = 0.2,
      zoom_range = 0.2,
      fill_mode = 'nearest'
      )
   
   processed_images = rescaler.flow_from_directory(
      dir,
      target_size = (224,224),
      follow_links = True,
      batch_size = 32, 
      
      shuffle = False, 
      )
   return processed_images
train_data = process_images_train(train_dir)
test_data = process_images_validate(validation_dir)



IMG_SHAPE =(224,224)
classifier_url = 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2' 

classifier = tf.keras.Sequential([
    hub.KerasLayer(classifier_url, input_shape=IMG_SHAPE+(3, )) # Channel 3 RGB
])

labels_path = tf.keras.utils.get_file('ImageNetLabels.txt', 
                                      'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')
imagenet_labels = np.array(open(labels_path).read().splitlines())

for image_batch, label_batch in test_data:
    print(f'Image batch shape: {image_batch.shape}')
    print(f'Label batch shape: {label_batch.shape}')
    break

result_batch = classifier.predict(image_batch)
print(f'Batch result shape: {result_batch.shape}')

predicted_class_names = imagenet_labels[np.argmax(result_batch, axis=-1)]
print(f'Batch predicted class names: {predicted_class_names}')

fig1 = plt.figure(figsize=(10, 9))
fig1.subplots_adjust(hspace=0.5)
for n in range(30):
    ax = fig1.add_subplot(6, 5, n+1)
    ax.imshow(image_batch[n])
    ax.set_title(predicted_class_names[n])
    ax.axis('off')
_ = fig1.suptitle('ImageNet predictions')

# Prepare transfer learning
## Download headless (without the top classification layer) model
feature_extractor_url = 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/2' #@param {type:"string"}
feature_extractor_layer = hub.KerasLayer(feature_extractor_url,
                                         input_shape=IMG_SHAPE+(3, ))
feature_batch = feature_extractor_layer(image_batch)
print(f'Feature vector shape: {feature_batch.shape}')

## Frozen feature extraction layer
feature_extractor_layer.trainable = False # for transfer learning classifier

## Make a model for classification
model = tf.keras.Sequential([
    feature_extractor_layer,
    layers.Dense(train_data.num_classes, activation='softmax')
])

## Check the model and prediction result
model.summary()

predictions = model(image_batch)
print(f'Prediction shape: {predictions.shape}')

# Train build
## Compile model for train
base_learning_rate = 0.001 # default
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'])

## Log class
### https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback
class CollectBatchStats(tf.keras.callbacks.Callback):
    def __init__(self):
        self.batch_losses = []
        self.batch_val_losses = []
        self.batch_acc = []
        self.batch_val_acc = []
    
    def on_epoch_end(self, epoch, logs=None):
        self.batch_losses.append(logs['loss'])
        self.batch_acc.append(logs['accuracy'])
        self.batch_val_losses.append(logs['val_loss'])
        self.batch_val_acc.append(logs['val_accuracy'])
        self.model.reset_metrics()

steps_per_epoch = np.ceil(train_data.samples/train_data.batch_size) # train all dataset per epoch
initial_epoch = 5
batch_stats_callback = CollectBatchStats()

history = model.fit(train_data,
                    epochs=initial_epoch,
                    steps_per_epoch=steps_per_epoch,
                    validation_data=test_data,
                    callbacks=[batch_stats_callback])

model.save('saved_model/my_model')

new_model = tf.keras.models.load_model('saved_model/my_model')

